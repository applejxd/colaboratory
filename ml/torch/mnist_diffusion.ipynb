{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN1EF4NI0Jc1V4No8q7yZq3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/applejxd/colaboratory/blob/master/ml/torch/mnist_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST で拡散モデル"
      ],
      "metadata": {
        "id": "Lv1fro7fpu8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデル定義"
      ],
      "metadata": {
        "id": "nckPnuKYpxmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 層の定義"
      ],
      "metadata": {
        "id": "TKCTeoWpp9Zy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aPJNMRR6leoE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResidualConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        '''\n",
        "        standard ResNet style convolutional block\n",
        "        '''\n",
        "        self.same_channels = in_channels==out_channels\n",
        "        self.is_res = is_res\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.is_res:\n",
        "            x1 = self.conv1(x)\n",
        "            x2 = self.conv2(x1)\n",
        "            # this adds on correct residual in case channels have increased\n",
        "            if self.same_channels:\n",
        "                out = x + x2\n",
        "            else:\n",
        "                out = x1 + x2\n",
        "            return out / 1.414\n",
        "        else:\n",
        "            x1 = self.conv1(x)\n",
        "            x2 = self.conv2(x1)\n",
        "            return x2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetDown(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UnetDown, self).__init__()\n",
        "        '''\n",
        "        process and downscale the image feature maps\n",
        "        '''\n",
        "        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "Bmd6W40FlkX0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetUp(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UnetUp, self).__init__()\n",
        "        '''\n",
        "        process and upscale the image feature maps\n",
        "        '''\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
        "            ResidualConvBlock(out_channels, out_channels),\n",
        "            ResidualConvBlock(out_channels, out_channels),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = torch.cat((x, skip), 1)\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QuKkP5CsltUR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbedFC(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim):\n",
        "        super(EmbedFC, self).__init__()\n",
        "        '''\n",
        "        generic one layer FC NN for embedding things\n",
        "        '''\n",
        "        self.input_dim = input_dim\n",
        "        layers = [\n",
        "            nn.Linear(input_dim, emb_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "HnSXWaz2lvNZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextUnet(nn.Module):\n",
        "    def __init__(self, in_channels, n_feat = 256, n_classes=10):\n",
        "        super(ContextUnet, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.n_feat = n_feat\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
        "\n",
        "        self.down1 = UnetDown(n_feat, n_feat)\n",
        "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
        "\n",
        "        self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
        "\n",
        "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
        "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
        "        self.contextembed1 = EmbedFC(n_classes, 2*n_feat)\n",
        "        self.contextembed2 = EmbedFC(n_classes, 1*n_feat)\n",
        "\n",
        "        self.up0 = nn.Sequential(\n",
        "            # nn.ConvTranspose2d(6 * n_feat, 2 * n_feat, 7, 7), # when concat temb and cemb end up w 6*n_feat\n",
        "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 7, 7), # otherwise just have 2*n_feat\n",
        "            nn.GroupNorm(8, 2 * n_feat),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
        "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
        "            nn.GroupNorm(8, n_feat),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, c, t, context_mask):\n",
        "        # x is (noisy) image, c is context label, t is timestep,\n",
        "        # context_mask says which samples to block the context on\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "        down1 = self.down1(x)\n",
        "        down2 = self.down2(down1)\n",
        "        hiddenvec = self.to_vec(down2)\n",
        "\n",
        "        # convert context to one hot embedding\n",
        "        c = nn.functional.one_hot(c, num_classes=self.n_classes).type(torch.float)\n",
        "\n",
        "        # mask out context if context_mask == 1\n",
        "        context_mask = context_mask[:, None]\n",
        "        context_mask = context_mask.repeat(1,self.n_classes)\n",
        "        context_mask = (-1*(1-context_mask)) # need to flip 0 <-> 1\n",
        "        c = c * context_mask\n",
        "\n",
        "        # embed context, time step\n",
        "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n",
        "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
        "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
        "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
        "\n",
        "        # could concatenate the context embedding here instead of adaGN\n",
        "        # hiddenvec = torch.cat((hiddenvec, temb1, cemb1), 1)\n",
        "\n",
        "        up1 = self.up0(hiddenvec)\n",
        "        # up2 = self.up1(up1, down2) # if want to avoid add and multiply embeddings\n",
        "        up2 = self.up1(cemb1*up1+ temb1, down2)  # add and multiply embeddings\n",
        "        up3 = self.up2(cemb2*up2+ temb2, down1)\n",
        "        out = self.out(torch.cat((up3, x), 1))\n",
        "        return out"
      ],
      "metadata": {
        "id": "CA1W3xZBlxNx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ddpm_schedules(beta1, beta2, T):\n",
        "    \"\"\"\n",
        "    Returns pre-computed schedules for DDPM sampling, training process.\n",
        "    \"\"\"\n",
        "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
        "\n",
        "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
        "    sqrt_beta_t = torch.sqrt(beta_t)\n",
        "    alpha_t = 1 - beta_t\n",
        "    log_alpha_t = torch.log(alpha_t)\n",
        "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
        "\n",
        "    sqrtab = torch.sqrt(alphabar_t)\n",
        "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
        "\n",
        "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
        "    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n",
        "\n",
        "    return {\n",
        "        \"alpha_t\": alpha_t,  # \\alpha_t\n",
        "        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n",
        "        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n",
        "        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n",
        "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
        "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
        "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n",
        "    }"
      ],
      "metadata": {
        "id": "m7QvoO70l3Aa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 拡散モデルの定義"
      ],
      "metadata": {
        "id": "Qqg9zwpxodFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DDPM(nn.Module):\n",
        "    def __init__(self, nn_model, betas, n_T, device, drop_prob=0.1):\n",
        "        super(DDPM, self).__init__()\n",
        "        self.nn_model = nn_model.to(device)\n",
        "\n",
        "        # register_buffer allows accessing dictionary produced by ddpm_schedules\n",
        "        # e.g. can access self.sqrtab later\n",
        "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
        "            self.register_buffer(k, v)\n",
        "\n",
        "        self.n_T = n_T\n",
        "        self.device = device\n",
        "        self.drop_prob = drop_prob\n",
        "        self.loss_mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        \"\"\"\n",
        "        this method is used in training, so samples t and noise randomly\n",
        "        \"\"\"\n",
        "\n",
        "        _ts = torch.randint(1, self.n_T+1, (x.shape[0],)).to(self.device)  # t ~ Uniform(0, n_T)\n",
        "        noise = torch.randn_like(x)  # eps ~ N(0, 1)\n",
        "\n",
        "        x_t = (\n",
        "            self.sqrtab[_ts, None, None, None] * x\n",
        "            + self.sqrtmab[_ts, None, None, None] * noise\n",
        "        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
        "        # We should predict the \"error term\" from this x_t. Loss is what we return.\n",
        "\n",
        "        # dropout context with some probability\n",
        "        context_mask = torch.bernoulli(torch.zeros_like(c)+self.drop_prob).to(self.device)\n",
        "\n",
        "        # return MSE between added noise, and our predicted noise\n",
        "        return self.loss_mse(noise, self.nn_model(x_t, c, _ts / self.n_T, context_mask))\n",
        "\n",
        "    def sample(self, n_sample, size, device, guide_w = 0.0):\n",
        "        # we follow the guidance sampling scheme described in 'Classifier-Free Diffusion Guidance'\n",
        "        # to make the fwd passes efficient, we concat two versions of the dataset,\n",
        "        # one with context_mask=0 and the other context_mask=1\n",
        "        # we then mix the outputs with the guidance scale, w\n",
        "        # where w>0 means more guidance\n",
        "\n",
        "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1), sample initial noise\n",
        "        c_i = torch.arange(0,10).to(device) # context for us just cycles throught the mnist labels\n",
        "        c_i = c_i.repeat(int(n_sample/c_i.shape[0]))\n",
        "\n",
        "        # don't drop context at test time\n",
        "        context_mask = torch.zeros_like(c_i).to(device)\n",
        "\n",
        "        # double the batch\n",
        "        c_i = c_i.repeat(2)\n",
        "        context_mask = context_mask.repeat(2)\n",
        "        context_mask[n_sample:] = 1. # makes second half of batch context free\n",
        "\n",
        "        x_i_store = [] # keep track of generated steps in case want to plot something\n",
        "        print()\n",
        "        for i in range(self.n_T, 0, -1):\n",
        "            print(f'sampling timestep {i}',end='\\r')\n",
        "            t_is = torch.tensor([i / self.n_T]).to(device)\n",
        "            t_is = t_is.repeat(n_sample,1,1,1)\n",
        "\n",
        "            # double batch\n",
        "            x_i = x_i.repeat(2,1,1,1)\n",
        "            t_is = t_is.repeat(2,1,1,1)\n",
        "\n",
        "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
        "\n",
        "            # split predictions and compute weighting\n",
        "            eps = self.nn_model(x_i, c_i, t_is, context_mask)\n",
        "            eps1 = eps[:n_sample]\n",
        "            eps2 = eps[n_sample:]\n",
        "            eps = (1+guide_w)*eps1 - guide_w*eps2\n",
        "            x_i = x_i[:n_sample]\n",
        "            x_i = (\n",
        "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
        "                + self.sqrt_beta_t[i] * z\n",
        "            )\n",
        "            if i%20==0 or i==self.n_T or i<8:\n",
        "                x_i_store.append(x_i.detach().cpu().numpy())\n",
        "\n",
        "        x_i_store = np.array(x_i_store)\n",
        "        return x_i, x_i_store"
      ],
      "metadata": {
        "id": "v63B1ZcCl6lv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルのインスタンス生成"
      ],
      "metadata": {
        "id": "hIuI0XdeoaNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_feat = 128 # 128 ok, 256 better (but slower)\n",
        "n_classes = 10\n",
        "n_T = 400 # 500\n",
        "device = \"cuda:0\"\n",
        "\n",
        "ddpm = DDPM(\n",
        "    nn_model=ContextUnet(in_channels=1, n_feat=n_feat, n_classes=n_classes),\n",
        "    betas=(1e-4, 0.02), n_T=n_T, device=device, drop_prob=0.1)\n",
        "ddpm.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfcQ-yx8mOwX",
        "outputId": "35bd2bf5-aa9b-4821-c280-c65689d8213d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DDPM(\n",
              "  (nn_model): ContextUnet(\n",
              "    (init_conv): ResidualConvBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): GELU(approximate='none')\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "    (down1): UnetDown(\n",
              "      (model): Sequential(\n",
              "        (0): ResidualConvBlock(\n",
              "          (conv1): Sequential(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "          (conv2): Sequential(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "        )\n",
              "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "    )\n",
              "    (down2): UnetDown(\n",
              "      (model): Sequential(\n",
              "        (0): ResidualConvBlock(\n",
              "          (conv1): Sequential(\n",
              "            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "          (conv2): Sequential(\n",
              "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "        )\n",
              "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "    )\n",
              "    (to_vec): Sequential(\n",
              "      (0): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
              "      (1): GELU(approximate='none')\n",
              "    )\n",
              "    (timeembed1): EmbedFC(\n",
              "      (model): Sequential(\n",
              "        (0): Linear(in_features=1, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (timeembed2): EmbedFC(\n",
              "      (model): Sequential(\n",
              "        (0): Linear(in_features=1, out_features=128, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (contextembed1): EmbedFC(\n",
              "      (model): Sequential(\n",
              "        (0): Linear(in_features=10, out_features=256, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (contextembed2): EmbedFC(\n",
              "      (model): Sequential(\n",
              "        (0): Linear(in_features=10, out_features=128, bias=True)\n",
              "        (1): GELU(approximate='none')\n",
              "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (up0): Sequential(\n",
              "      (0): ConvTranspose2d(256, 256, kernel_size=(7, 7), stride=(7, 7))\n",
              "      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (up1): UnetUp(\n",
              "      (model): Sequential(\n",
              "        (0): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "        (1): ResidualConvBlock(\n",
              "          (conv1): Sequential(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "          (conv2): Sequential(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "        )\n",
              "        (2): ResidualConvBlock(\n",
              "          (conv1): Sequential(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "          (conv2): Sequential(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (up2): UnetUp(\n",
              "      (model): Sequential(\n",
              "        (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "        (1): ResidualConvBlock(\n",
              "          (conv1): Sequential(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "          (conv2): Sequential(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "        )\n",
              "        (2): ResidualConvBlock(\n",
              "          (conv1): Sequential(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "          (conv2): Sequential(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (out): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
              "      (2): ReLU()\n",
              "      (3): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (loss_mse): MSELoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習設定"
      ],
      "metadata": {
        "id": "K2mryrNip5Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optionally load a model\n",
        "# ddpm.load_state_dict(torch.load(\"./data/diffusion_outputs/ddpm_unet01_mnist_9.pth\"))\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 256\n",
        "lrate = 1e-4\n",
        "\n",
        "tf = transforms.Compose([transforms.ToTensor()]) # mnist is already normalised 0 to 1\n",
        "\n",
        "dataset = MNIST(\"./data\", train=True, download=True, transform=tf)\n",
        "dataloader = DataLoader(\n",
        "    dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "optim = torch.optim.Adam(ddpm.parameters(), lr=lrate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8v4cXBImwgW",
        "outputId": "78b34cce-2ba5-42ba-d92f-49553ee92453"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 51662698.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 125659433.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 19516128.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4419050.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "n_epoch = 20\n",
        "\n",
        "def train(ddpm, device, dataloader, optim, epoch):\n",
        "    global n_epoch\n",
        "\n",
        "    ddpm.train()\n",
        "    # linear lrate decay\n",
        "    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
        "    pbar = tqdm(dataloader)\n",
        "    loss_ema = None\n",
        "    for x, c in pbar:\n",
        "        optim.zero_grad()\n",
        "        x = x.to(device)\n",
        "        c = c.to(device)\n",
        "        loss = ddpm(x, c)\n",
        "        loss.backward()\n",
        "\n",
        "        if loss_ema is None:\n",
        "            loss_ema = loss.item()\n",
        "        else:\n",
        "            loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n",
        "\n",
        "        pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
        "        optim.step()\n",
        "    return x, c"
      ],
      "metadata": {
        "id": "ERPdzIfbqPIJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "def create_image(x_gen, x, c, w, ep, n_sample, save_dir):\n",
        "    # append some real images at bottom, order by class also\n",
        "    x_real = torch.Tensor(x_gen.shape).to(device)\n",
        "    for k in range(n_classes):\n",
        "        for j in range(int(n_sample/n_classes)):\n",
        "            try:\n",
        "                idx = torch.squeeze((c == k).nonzero())[j]\n",
        "            except:\n",
        "                idx = 0\n",
        "            x_real[k+(j*n_classes)] = x[idx]\n",
        "\n",
        "    x_all = torch.cat([x_gen, x_real])\n",
        "    grid = make_grid(x_all*-1 + 1, nrow=10)\n",
        "    save_image(grid, save_dir + f\"image_ep{ep}_w{w}.png\")\n",
        "    print('saved image at ' + save_dir + f\"image_ep{ep}_w{w}.png\")"
      ],
      "metadata": {
        "id": "JI71wXjhzmwv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def animate_diff(axs, n_sample, i, x_gen_store):\n",
        "    print(f'gif animating frame {i} of {x_gen_store.shape[0]}', end='\\r')\n",
        "    plots = []\n",
        "    for row in range(int(n_sample/n_classes)):\n",
        "        for col in range(n_classes):\n",
        "            axs[row, col].clear()\n",
        "            axs[row, col].set_xticks([])\n",
        "            axs[row, col].set_yticks([])\n",
        "            # plots.append(axs[row, col].imshow(x_gen_store[i,(row*n_classes)+col,0],cmap='gray'))\n",
        "            plots.append(\n",
        "                axs[row, col].imshow(\n",
        "                    -x_gen_store[i,(row*n_classes)+col,0],\n",
        "                    cmap='gray',vmin=(-x_gen_store[i]).min(),\n",
        "                    vmax=(-x_gen_store[i]).max()))\n",
        "    return plots"
      ],
      "metadata": {
        "id": "lz0BeErvz27F"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "\n",
        "def create_gif(x_gen_store, ep, w, n_sample, save_dir):\n",
        "    # create gif of images evolving over time, based on x_gen_store\n",
        "    fig, axs = plt.subplots(\n",
        "        nrows=int(n_sample/n_classes),\n",
        "        ncols=n_classes,\n",
        "        sharex=True,\n",
        "        sharey=True,\n",
        "        figsize=(8,3))\n",
        "\n",
        "    ani = FuncAnimation(\n",
        "        fig, lambda x, y: animate_diff(axs, n_sample, x, y), fargs=[x_gen_store],\n",
        "        interval=200, blit=False, repeat=True,\n",
        "        frames=x_gen_store.shape[0])\n",
        "    ani.save(\n",
        "        save_dir + f\"gif_ep{ep}_w{w}.gif\",\n",
        "        dpi=100, writer=PillowWriter(fps=5))\n",
        "    print('saved image at ' + save_dir + f\"gif_ep{ep}_w{w}.gif\")"
      ],
      "metadata": {
        "id": "wUeGJKDv0ooz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(ddpm, device, x, c, ep, save_dir):\n",
        "    # for eval, save an image of currently generated samples (top rows)\n",
        "    # followed by real images (bottom rows)\n",
        "    ddpm.eval()\n",
        "    with torch.no_grad():\n",
        "        n_sample = 4*n_classes\n",
        "        ws_test = [0.0, 0.5, 2.0] # strength of generative guidance\n",
        "        for w_i, w in enumerate(ws_test):\n",
        "            x_gen, x_gen_store = ddpm.sample(\n",
        "                n_sample, (1, 28, 28), device, guide_w=w)\n",
        "            create_image(x_gen, x, c, w, ep, n_sample, save_dir)\n",
        "\n",
        "        if ep%5==0 or ep == int(n_epoch-1):\n",
        "            create_gif(x_gen_store, ep, w, n_sample, save_dir)"
      ],
      "metadata": {
        "id": "GgZs9U2irAqL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習"
      ],
      "metadata": {
        "id": "ZDveDWOSp2QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Tuple\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# hardcoding these here\n",
        "save_model = False\n",
        "save_dir = './data/diffusion_outputs10/'\n",
        "\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "for ep in range(n_epoch):\n",
        "    print(f'epoch {ep}')\n",
        "    x, c = train(ddpm, device, dataloader, optim, ep)\n",
        "\n",
        "    if ep == 0 or ep == n_epoch-1:\n",
        "        test(ddpm, device, x, c, ep, save_dir)\n",
        "\n",
        "    # optionally save model\n",
        "    if save_model and ep == int(n_epoch-1):\n",
        "        torch.save(ddpm.state_dict(), save_dir + f\"model_{ep}.pth\")\n",
        "        print('saved model at ' + save_dir + f\"model_{ep}.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRa0X6hHmELy",
        "outputId": "7538e26e-7f80-40ce-e754-65f60c598d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.0319:  31%|███       | 72/235 [00:34<01:18,  2.06it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8etZjsZsBCUU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}